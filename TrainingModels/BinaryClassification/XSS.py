#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
train_xss_models.py

Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh ph√°t hi·ªán XSS payloads.
L∆∞u CountVectorizer + MultinomialNB ƒë·ªÉ d√πng cho WAF runtime.

Usage:
    python train_xss_models.py
"""

import os
import sys
import joblib
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, StackingClassifier
)

# ======================== #
#       HELPER FUNCS       #
# ======================== #
def find_xss_dataset():
    """T√¨m file dataset XSS trong project."""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        # 1. N·∫øu ch·∫°y t·ª´ BinaryClassification/, l√πi ra 1 c·∫•p -> data/processed
        os.path.join(script_dir, '..', 'data', 'processed', 'processed_payloads.csv'),

        # 2. N·∫øu ch·∫°y t·ª´ th∆∞ m·ª•c g·ªëc project
        os.path.join(script_dir, 'data', 'processed', 'processed_payloads.csv'),

        # 3. N·∫øu ch·∫°y t·ª´ s√¢u h∆°n (ph√≤ng tr∆∞·ªùng h·ª£p kh√°c)
        os.path.join(script_dir, '..', '..', 'data', 'processed', 'processed_payloads.csv'),
    ]
    for p in candidates:
        if os.path.exists(p):
            return os.path.abspath(p)
    raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file XSS dataset. ƒê√£ th·ª≠:\n" + "\n".join(candidates))


def custom_tokenizer(text):
    """Tokenizer c∆° b·∫£n: t√°ch theo kho·∫£ng tr·∫Øng."""
    return text.split()


def ensure_dir(path):
    d = os.path.dirname(path)
    if d and not os.path.exists(d):
        os.makedirs(d, exist_ok=True)


def save_artifacts(nb_clf, vectorizer, base_save_dir):
    """L∆∞u model NB + vectorizer."""
    ensure_dir(base_save_dir)
    nb_path = os.path.join(base_save_dir, "nb_classifier_xss.pkl")
    vect_path = os.path.join(base_save_dir, "count_vectorizer_xss.pkl")
    joblib.dump(nb_clf, nb_path)
    joblib.dump(vectorizer, vect_path)
    print(f"‚úÖ Saved NB model to: {nb_path}")
    print(f"‚úÖ Saved vectorizer to: {vect_path}")
    return nb_path, vect_path


# ======================== #
#          MAIN            #
# ======================== #
def main():
    print("üîç Looking for XSS dataset...")
    try:
        df_path = find_xss_dataset()
    except FileNotFoundError as ex:
        print("‚ùå ERROR:", ex)
        sys.exit(1)

    print("üìÇ Loading dataset from:", df_path)
    df = pd.read_csv(df_path, dtype=str, keep_default_na=False)

    # Chu·∫©n h√≥a c·ªôt
    cols = [c.lower() for c in df.columns]
    payload_col = df.columns[cols.index('payload')] if 'payload' in cols else df.columns[0]
    label_col = df.columns[cols.index('is_malicious')] if 'is_malicious' in cols else df.columns[1]

    df = df[[payload_col, label_col]].copy()
    df.columns = ['payload', 'is_malicious']

    def to_int_lbl(x):
        try:
            return int(float(x))
        except Exception:
            return 1 if str(x).strip() else 0

    df['payload'] = df['payload'].astype(str).fillna('').apply(lambda s: s.strip())
    df['is_malicious'] = df['is_malicious'].apply(to_int_lbl).astype(int)
    df = df[df['payload'] != ''].drop_duplicates(subset=['payload'])

    print("üìä Dataset size after cleaning:", len(df))
    print("Label distribution:")
    print(df['is_malicious'].value_counts())

    # Ch·ªâ gi·ªØ nh√£n h·ª£p l·ªá
    df = df[df['is_malicious'].isin([0, 1])]

    # Vectorization
    vectorizer = CountVectorizer(min_df=1, tokenizer=custom_tokenizer, ngram_range=(1, 3))
    X = vectorizer.fit_transform(df['payload'])
    y = df['is_malicious'].values

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # ========================
    # TRAINING MULTIPLE MODELS
    # ========================
    print("\nTraining MultinomialNB...")
    nb = MultinomialNB().fit(X_train, y_train)
    y_pred = nb.predict(X_test)
    print("NaiveBayes Accuracy:", accuracy_score(y_test, y_pred))
    print("NaiveBayes Classification Report:\n", classification_report(y_test, y_pred))

    print("\nTraining LogisticRegression...")
    lr = LogisticRegression(max_iter=2000).fit(X_train, y_train)
    y_pred_lr = lr.predict(X_test)
    print("Logistic Accuracy:", accuracy_score(y_test, y_pred_lr))
    print("Logistic Classification Report:\n", classification_report(y_test, y_pred_lr))

    print("\nTraining RandomForest...")
    rf = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    print("RandomForest Accuracy:", accuracy_score(y_test, y_pred_rf))
    print("RandomForest Classification Report:\n", classification_report(y_test, y_pred_rf))

    # ========================
    # SAVE NB MODEL
    # ========================
    save_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "saved_models", "xss")
    save_artifacts(nb, vectorizer, save_dir)
    print("\n‚úÖ Training completed successfully.")


if __name__ == "__main__":
    main()
